{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52602ebc-e32f-453c-8280-e56eeec93955",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp38-cp38-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from datasets) (14.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from datasets) (3.8.6)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Using cached datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Downloading regex-2023.10.3-cp38-cp38-macosx_11_0_arm64.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp38-cp38-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.1/426.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp38-cp38-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp38-cp38-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=cff0bac6b4992a2fdbf7c3535ffe825f0968e66e969073e2dc1b8d2fd7851255\n",
      "  Stored in directory: /Users/drpawar/Library/Caches/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow-hotfix, dill, click, absl-py, responses, nltk, multiprocess, huggingface-hub, tokenizers, rouge_score, transformers, datasets, evaluate\n",
      "Successfully installed absl-py-2.0.0 click-8.1.7 datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 pyarrow-hotfix-0.6 regex-2023.10.3 responses-0.18.0 rouge_score-0.1.2 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d632a8-f3b7-4213-acb1-af9a995557f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0438830c-c710-4136-867e-7f11e08feed6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (2.1.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (1.5.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (4.35.2)\n",
      "Requirement already satisfied: numpy in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (1.24.4)\n",
      "Requirement already satisfied: requests in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (4.66.1)\n",
      "Requirement already satisfied: matplotlib in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (3.7.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from bert_score) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
      "Requirement already satisfied: filelock in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.0.0->bert_score) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from transformers>=3.0.0->bert_score) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from matplotlib->bert_score) (6.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->bert_score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->bert_score) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->bert_score) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->bert_score) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Installing collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a622683e-6041-47eb-b01f-e450f6fba553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import multiprocessing as mp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2988aa-7e8a-4c7b-887b-eb83c96b68c7",
   "metadata": {},
   "source": [
    "### Q1 : Use a pre-trained google/flan-t5-small as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39eb9c36-1d94-4d0a-b056-fb53fcb96741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2552ca-7890-4ff4-8fa2-64a9370089d0",
   "metadata": {},
   "source": [
    "### Q2 : Verify if the summariza'on task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "913b8b45-40fe-4f8f-b780-67cceedc5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_test_billsum = load_dataset(\"billsum\", split='ca_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "732fde55-d92d-46a3-a06e-18d1958099ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'title'],\n",
       "    num_rows: 1237\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_test_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdd6e574-b00d-4475-81ce-646eba6d9257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe8c29db864459d92a6f77ee532e8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    x['text'] = ['summarize: ' + xi for xi in x['text']]\n",
    "    return x\n",
    "\n",
    "ca_test_billsum = ca_test_billsum.map(lambda x: preprocess(x), batched=True, num_proc=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46de07cf-4d3a-4b6a-aa79-77ccc91addbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b51ec315774e5da74a5dbb46474b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ca_test_billsum = ca_test_billsum.map(lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=1692, \n",
    "                                                          return_tensors=\"pt\"), batched=True, num_proc=mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49eed53f-8f9e-4e2d-b173-466bca41eb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84115b98fa86433a8369c815e8179069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(x):\n",
    "    \n",
    "    x['output'] = [model.generate(torch.tensor(x_ids).reshape(1, -1), \n",
    "                                  max_new_tokens=100, \n",
    "                                  do_sample=False)[0] for x_ids in x['input_ids']]\n",
    "    return x\n",
    "    \n",
    "ca_test_billsum = ca_test_billsum.map(lambda x: generate(x), batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9a98dc7-9177-4f34-bff1-bc59d7cd61b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ae6f61d1e64a7085869f18a9da689f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode(x):\n",
    "    x['generated_summary'] = [tokenizer.decode(torch.tensor(xo), skip_special_tokens=True) for xo in x['output']]\n",
    "    return x\n",
    "\n",
    "ca_test_billsum = ca_test_billsum.map(lambda x: decode(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9670fce-3fd1-4bd8-8b70-27e959199eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'output', 'generated_summary'],\n",
       "    num_rows: 1237\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_test_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2030b069-e715-430e-a8fa-a204a1710426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fba0638-d035-4f28-8b3d-ba0aa2841095",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = metric.compute(predictions=ca_test_billsum['generated_summary'], references=ca_test_billsum['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d3949fa-987d-4e18-b9b5-3e679762c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = {}\n",
    "for k, v in metric_dict.items():\n",
    "    lp = 'L:' + str(round(v.low.precision, 3))\n",
    "    mp = 'M:' + str(round(v.mid.precision, 3))\n",
    "    hp = 'H:' + str(round(v.high.precision, 3))\n",
    "    ps = ', '.join([lp, mp, hp])\n",
    "\n",
    "    lr = 'L:' + str(round(v.low.recall, 3))\n",
    "    mr = 'M:' + str(round(v.mid.recall, 3))\n",
    "    hr = 'H:' + str(round(v.high.recall, 3))\n",
    "    rs = ', '.join([lr, mr, hr])\n",
    "\n",
    "    lf = 'L:' + str(round(v.low.fmeasure, 3))\n",
    "    mf = 'M:' + str(round(v.mid.fmeasure, 3))\n",
    "    hf = 'H:' + str(round(v.high.fmeasure, 3))\n",
    "    fs = ', '.join([lf, mf, hf])\n",
    "\n",
    "    md[k] = [ps, rs, fs]\n",
    "pdf = pd.DataFrame(md, index=['precision', 'recall', 'f-measure']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec178aa4-1e35-4c7c-9428-b15707631bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>L:0.535, M:0.55, H:0.566</td>\n",
       "      <td>L:0.185, M:0.197, H:0.209</td>\n",
       "      <td>L:0.429, M:0.444, H:0.46</td>\n",
       "      <td>L:0.477, M:0.493, H:0.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>L:0.08, M:0.084, H:0.09</td>\n",
       "      <td>L:0.033, M:0.036, H:0.039</td>\n",
       "      <td>L:0.057, M:0.06, H:0.064</td>\n",
       "      <td>L:0.067, M:0.071, H:0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f-measure</th>\n",
       "      <td>L:0.124, M:0.131, H:0.138</td>\n",
       "      <td>L:0.052, M:0.056, H:0.06</td>\n",
       "      <td>L:0.089, M:0.094, H:0.098</td>\n",
       "      <td>L:0.105, M:0.111, H:0.116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              rouge1                     rouge2  \\\n",
       "precision   L:0.535, M:0.55, H:0.566  L:0.185, M:0.197, H:0.209   \n",
       "recall       L:0.08, M:0.084, H:0.09  L:0.033, M:0.036, H:0.039   \n",
       "f-measure  L:0.124, M:0.131, H:0.138   L:0.052, M:0.056, H:0.06   \n",
       "\n",
       "                              rougeL                  rougeLsum  \n",
       "precision   L:0.429, M:0.444, H:0.46  L:0.477, M:0.493, H:0.507  \n",
       "recall      L:0.057, M:0.06, H:0.064  L:0.067, M:0.071, H:0.075  \n",
       "f-measure  L:0.089, M:0.094, H:0.098  L:0.105, M:0.111, H:0.116  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3fa2268-e898-4f43-ae6d-741d5ad0ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summarys_len = [len(gs) for gs in ca_test_billsum['generated_summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a1a0c8b-35dd-4ff7-a015-b7df1b43ef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237.56750202101858, 187.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(generated_summarys_len), np.median(generated_summarys_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "530bfd73-0236-4c9c-ba94-019ec6c2b58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2168.5828617623283, 1894.0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarys_len = [len(gs) for gs in ca_test_billsum['summary']]\n",
    "\n",
    "np.average(summarys_len), np.median(summarys_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f30ab3-a3c6-42af-bc21-6a02327e5dda",
   "metadata": {},
   "source": [
    "### Q3: Verify if the Q&A task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "379cc23e-f9e9-45cc-9d04-fe45a1e8588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split='validation[0:1000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a3a42da5-9101-4a88-8e24-ae00ebe69f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "479ee420-5dd5-47f1-b0ac-cfa2c5e6e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "80ab95fe-2cc6-44e0-8b9a-626f305ef3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33386d561c7d488f8884a91b1d15d8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    \n",
    "    x['prompt'] = ['Given a question and context, Answer the question using context. Question: ' + xi[0] + ' Context: ' + xi[1]  \n",
    "                 for xi in list(zip(x['question'],  x['context']))]\n",
    "    return x\n",
    "\n",
    "squad = squad.map(lambda x: preprocess(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e16e5fa3-57d6-4123-bb18-9b601b126bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb4f81fd3d14c269287f4103ca48c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = squad.map(lambda x: tokenizer(x['prompt'], padding=True, truncation=True, \n",
    "                                      return_tensors=\"pt\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fce00b29-afe9-432b-a106-019d6d893928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792b536af1074514b41571f67a6c7c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(x):\n",
    "    \n",
    "    x['output'] = [model.generate(torch.tensor(x_ids).reshape(1, -1), \n",
    "                                  max_new_tokens=100, \n",
    "                                  do_sample=False)[0] for x_ids in x['input_ids']]\n",
    "    return x\n",
    "    \n",
    "squad = squad.map(lambda x: generate(x), batched=True, batch_size=8)\n",
    "\n",
    "# def apply(x):\n",
    "#     x['output'] = [model(torch.tensor(x_ids).reshape(1, -1)) for x_ids in x['input_ids']]\n",
    "#     return x\n",
    "    \n",
    "# squad = squad.map(lambda x: apply(x), batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "684ff2ca-9e32-4118-85ff-f60b96bb6952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466b7385a110489ea134c19279bc8251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode(x):\n",
    "    x['predicted_answers'] = [tokenizer.decode(torch.tensor(xo), skip_special_tokens=True) for xo in x['output']]\n",
    "    return x\n",
    "\n",
    "squad = squad.map(lambda x: decode(x), batched=True)\n",
    "\n",
    "# def decode(x):\n",
    "\n",
    "#     pa = []\n",
    "#     for i, xo in enumerate(x['output']):\n",
    "#         answer_start_index = np.argmax(xo['start_logits'])\n",
    "#         answer_end_index = np.argmax(xo['end_logits'])\n",
    "#         inputs = x['input_ids'][i]\n",
    "#         predict_answer_tokens = inputs[answer_start_index : answer_end_index + 1]\n",
    "#         pa.append(tokenizer.decode(predict_answer_tokens))   \n",
    "#     x['predicted_answers'] = pa\n",
    "#     return x\n",
    "\n",
    "# squad = squad.map(lambda x: decode(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6ac3b302-8639-42d9-b98a-36e2cbc79012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "squad_metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "82846c3c-1e40-4827-8e10-9135b41f0f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 68.0, 'f1': 73.30533849203195}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [{'id': t[0], 'prediction_text':t[1]} for t in list(zip(squad['id'], squad['predicted_answers']))]\n",
    "references = [{'id': t[0], 'answers':t[1]} for t in list(zip(squad['id'], squad['answers']))]\n",
    "\n",
    "squad_metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7111f99-4ac9-4556-93bd-234f6a505913",
   "metadata": {},
   "source": [
    "scores suggest that the Flan T5 Small pretrained model performs reasonably well on the SQuAD validation set, but there is still room for improvement, especially in achieving exact matches for a higher percentage of questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99203c07-adc7-4199-a880-141bfb1aee21",
   "metadata": {},
   "source": [
    "### Q4: Verify if English to French transla'on task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0d8272fb-952d-47a2-a616-0997a6d6a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\", split='train[0:500]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e7bef07e-b2ec-4224-845b-85461afa24d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ab09e274-f525-4187-a0cb-0b8d63446050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "454b41c2-f13c-44c8-8df6-6489249ca348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5733f5b8eb48518d2441c08a8a1fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    \n",
    "    x['prompt'] = ['Translate given english text to french. Given English Text: ' + xi['en'] for xi in x['translation']]\n",
    "    return x\n",
    "\n",
    "books = books.map(lambda x: preprocess(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cacc4d25-3e61-4b0f-9133-3e514f57a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb1bebd267749988a7d695d193224b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = books.map(lambda x: tokenizer(x['prompt'], padding=True, truncation=True, \n",
    "                                      return_tensors=\"pt\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "21f4dde1-8d9e-484a-b704-f507829c9c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e67d11528b4d63b985650b698181de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = books.map(lambda x: generate(x), batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bd2d8b5c-7f26-4bbd-aa34-662ea8569dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017628ef81444ef99ddc6cf83595dd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = books.map(lambda x: decode(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f73aba35-a91b-40db-8f2a-71f38d905231",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from sacrebleu) (2023.10.3)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from sacrebleu) (1.24.4)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-4.9.3.tar.gz (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: lxml\n",
      "  Building wheel for lxml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lxml: filename=lxml-4.9.3-cp38-cp38-macosx_11_0_arm64.whl size=1606190 sha256=6b27c9f6bd87c3d64a6ff903aee150127bfeb0719d9fd3e565890cc4eeaa0602\n",
      "  Stored in directory: /Users/drpawar/Library/Caches/pip/wheels/70/25/e4/422c8f9cd5b754007088081b24e9bb2a96d85d66c3bb67d413\n",
      "Successfully built lxml\n",
      "Installing collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 lxml-4.9.3 portalocker-2.8.2 sacrebleu-2.3.2 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "768b04fe-7248-4501-92eb-47b358f84f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9ea6b661-d8e4-4a93-87a8-7e55f76a7003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation', 'prompt', 'input_ids', 'attention_mask', 'output', 'predicted_answers'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "963e6fed-a766-4322-a8ba-aa3027ec0064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637720f436ba4ed1aa441e965ca9d288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_actual_translations(x):\n",
    "    x['actual_translation'] = [t['fr'] for t in x['translation']]\n",
    "    return x\n",
    "    \n",
    "\n",
    "books = books.map(lambda x: get_actual_translations(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8cd7d102-a320-4c66-8347-c65d3fd4def0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Première partie', 'PREMIÈRE PARTIE')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['predicted_answers'][2], books['actual_translation'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "38a6156c-67c5-448c-a6d6-ea8ac7bdb730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.012295245174718,\n",
       " 'counts': [2980, 723, 217, 75],\n",
       " 'totals': [10894, 10417, 9948, 9489],\n",
       " 'precisions': [27.354507068110888,\n",
       "  6.940577901507152,\n",
       "  2.1813429835142744,\n",
       "  0.7903888713246917],\n",
       " 'bp': 0.943290711629967,\n",
       " 'sys_len': 10894,\n",
       " 'ref_len': 11530}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=books['predicted_answers'], references=[[at] for at in books['actual_translation']], \n",
    "               lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e8458-2e2a-44a1-bd2c-41df5ed6d26a",
   "metadata": {},
   "source": [
    "The BLEU score of 4.0123 suggests that the Flan T5 Small pretrained model achieves a moderate level of performance in English to French translation.\n",
    "\n",
    "The n-gram precisions show that the model performs better at capturing individual words (unigrams) than longer phrases.\n",
    "\n",
    "The brevity penalty is close to 1, indicating that the lengths of the generated and reference translations are reasonably well-matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3f254-2ffe-402f-8437-7006bbaec97c",
   "metadata": {},
   "source": [
    "### Q5: Programma'cally print the names of all the model layers and their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6ef5c20f-8b44-4eff-9104-b90732706a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "09b1dc44-eaeb-4012-9cca-81660bde9e9d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: shared.weight, Shape: torch.Size([32128, 512])\n",
      "Layer: encoder.block.0.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.0.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.0.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.0.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Shape: torch.Size([32, 6])\n",
      "Layer: encoder.block.0.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.0.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.0.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.1.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.1.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.1.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.1.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.1.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.1.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.1.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.2.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.2.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.2.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.2.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.2.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.2.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.2.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.3.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.3.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.3.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.3.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.3.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.3.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.3.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.4.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.4.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.4.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.4.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.4.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.4.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.4.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.5.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.5.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.5.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.5.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.5.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.5.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.5.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.6.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.6.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.6.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.6.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.6.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.6.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.6.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.7.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.7.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.7.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: encoder.block.7.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: encoder.block.7.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: encoder.block.7.layer.1.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: encoder.block.7.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: encoder.final_layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.0.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.0.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.0.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.0.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Shape: torch.Size([32, 6])\n",
      "Layer: decoder.block.0.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.0.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.0.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.0.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.0.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.0.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.0.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.0.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.1.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.1.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.1.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.1.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.1.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.1.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.1.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.1.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.1.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.1.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.1.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.1.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.2.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.2.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.2.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.2.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.2.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.2.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.2.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.2.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.2.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.2.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.2.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.2.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.3.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.3.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.3.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.3.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.3.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.3.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.3.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.3.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.3.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.3.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.3.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.3.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.4.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.4.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.4.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.4.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.4.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.4.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.4.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.4.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.4.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.4.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.4.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.4.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.5.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.5.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.5.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.5.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.5.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.5.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.5.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.5.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.5.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.5.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.5.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.5.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.6.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.6.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.6.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.6.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.6.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.6.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.6.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.6.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.6.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.6.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.6.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.6.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.7.layer.0.SelfAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.7.layer.0.SelfAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.7.layer.0.SelfAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.7.layer.0.SelfAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.7.layer.0.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.7.layer.1.EncDecAttention.q.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.7.layer.1.EncDecAttention.k.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.7.layer.1.EncDecAttention.v.weight, Shape: torch.Size([384, 512])\n",
      "Layer: decoder.block.7.layer.1.EncDecAttention.o.weight, Shape: torch.Size([512, 384])\n",
      "Layer: decoder.block.7.layer.1.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Shape: torch.Size([1024, 512])\n",
      "Layer: decoder.block.7.layer.2.DenseReluDense.wo.weight, Shape: torch.Size([512, 1024])\n",
      "Layer: decoder.block.7.layer.2.layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: decoder.final_layer_norm.weight, Shape: torch.Size([512])\n",
      "Layer: lm_head.weight, Shape: torch.Size([32128, 512])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Shape: {param.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3727b9bd-cbef-4484-9c3c-61b289a34096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 76961152\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2088ee88-fee9-421d-bd18-09806c366401",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before final layer weights update:\n",
      "  Parameter containing:\n",
      "tensor([ 1.5583e-01,  1.6458e-01,  1.8197e-01,  2.0792e-01,  1.5886e-01,\n",
      "         1.4222e-01,  1.5845e-01,  1.4269e-01,  1.3648e-01,  1.5702e-01,\n",
      "         1.6670e-01,  1.3271e-01,  1.7980e-01,  3.2683e-01,  2.0897e-01,\n",
      "         2.6234e-01,  1.8381e-01,  1.8566e-01,  1.8115e-01,  1.9588e-01,\n",
      "         1.5456e-01,  2.1353e-01,  1.5126e-01,  1.6348e-01,  1.8062e-01,\n",
      "         1.4414e-01,  1.7974e-01,  2.0646e-01,  1.7899e-01,  2.0434e-01,\n",
      "         1.6415e-01,  1.4987e-01,  1.3866e-01,  2.2488e-01,  1.7041e-01,\n",
      "         6.1698e-01,  1.8228e-01,  1.7578e-01,  1.6113e-01,  2.4024e-01,\n",
      "         1.6280e-01,  2.2871e-01,  1.6127e-01,  1.8426e-01,  2.1641e-01,\n",
      "         2.6774e-01,  1.8475e-01,  1.5955e-01,  2.5002e-01,  1.9592e-01,\n",
      "         1.5467e-01,  2.0025e-01,  1.7020e-01,  1.4393e-01,  1.9788e-01,\n",
      "         1.5900e-01,  1.4895e-01,  1.5042e-01,  2.6026e-01,  1.5933e-01,\n",
      "         1.5081e-01,  2.0102e-01,  1.9843e-01,  1.5577e-01,  1.5257e-01,\n",
      "         1.5654e-01,  1.6762e-01,  7.5296e-01,  1.6636e-01,  1.5402e-01,\n",
      "         4.6294e-02,  1.6461e-01, -6.6102e-03,  1.7538e-01,  1.5686e-01,\n",
      "         2.5396e-01,  1.9640e-01,  2.0717e-01,  2.0110e-01,  2.0370e-01,\n",
      "         2.1669e-01,  1.6539e-01,  1.6955e-01,  1.2699e-01,  1.4506e-01,\n",
      "         1.7137e-01,  1.8019e-01,  1.7089e-01,  1.6468e-01,  2.1281e-01,\n",
      "         1.7568e-01,  1.3533e-01,  1.5223e-01,  1.4241e-01,  1.4643e-01,\n",
      "         2.1323e-01,  1.6563e-01,  3.7506e-01,  1.3279e-01,  1.4821e-01,\n",
      "         1.6229e-01,  1.5335e-01,  1.9785e-01,  1.5253e-01,  1.5380e-01,\n",
      "         1.8114e-01,  1.5337e-01,  1.5664e-01,  1.6879e-01,  1.5306e-01,\n",
      "         1.9569e-01,  1.5582e-01,  1.8156e-01,  2.0031e-01,  1.4573e-01,\n",
      "         1.6565e-01,  1.7020e-01,  1.5614e-01,  1.4373e-01,  1.7403e-01,\n",
      "         1.5083e-01,  1.5831e-01,  1.6520e-01,  1.5690e-01,  1.9232e-01,\n",
      "         1.5903e-01,  2.0464e-01,  1.4832e-01,  1.7730e-01,  1.8827e-01,\n",
      "         1.4112e-01,  1.9067e-01,  1.6094e-01,  1.4695e-01,  1.5871e-01,\n",
      "         1.6182e-01,  2.0271e-01,  1.8662e-01,  2.0484e-01,  1.4663e-01,\n",
      "         1.8820e-01,  2.1564e-01,  1.7099e-01,  2.4158e-01,  1.9468e-01,\n",
      "         2.7900e-01,  1.8111e-01,  2.3911e-01,  1.5275e-01,  1.6435e-01,\n",
      "         1.6918e-01,  1.7360e-01,  1.4911e-01,  1.4707e-01,  2.3178e-01,\n",
      "         1.3859e-01,  2.2257e-01,  1.7704e-01,  1.8228e-01,  2.3528e-01,\n",
      "         2.8649e-01,  1.8691e-01,  2.0105e-01,  1.5032e-01,  1.7764e-01,\n",
      "         1.6979e-01,  1.4514e-01,  1.3593e-01,  1.2960e-01,  1.5276e-01,\n",
      "         1.8807e-01,  1.7210e-01,  2.1407e-01,  2.1323e-01,  2.3099e-01,\n",
      "         1.4930e-01,  2.0003e-01,  2.2725e-01,  1.4609e-01,  3.5971e-01,\n",
      "         1.4036e-01,  1.7294e-01,  1.4863e-01,  1.9655e-01,  1.4858e-01,\n",
      "         2.5492e-01,  2.2977e-01,  1.2809e-01,  1.3660e-01,  2.7187e-01,\n",
      "         1.8013e-01,  2.6223e-01,  1.4699e-01,  2.1151e+00,  1.5537e-01,\n",
      "         1.6117e-01,  1.8694e-01,  1.8826e-01,  1.4469e-01,  1.9167e-01,\n",
      "         2.0336e-01,  1.5928e-01,  2.0197e-01,  1.4118e-01,  2.0147e-01,\n",
      "         1.8554e-01,  1.9080e-01,  1.6589e-01,  2.3433e-01,  1.4772e-01,\n",
      "         1.9256e-01,  1.6619e-01,  1.5463e-01,  1.4645e-01,  2.2029e-01,\n",
      "         1.9336e-01,  1.7481e-01,  1.7506e-01,  2.1029e-01,  2.3300e-01,\n",
      "         1.8893e-01,  1.7255e-01,  1.3724e-01,  2.0819e-01,  1.6483e-01,\n",
      "         1.5570e-01,  1.4373e-01,  1.5941e-01,  1.5687e-01,  2.1235e-01,\n",
      "         1.5520e-01,  2.5171e+00,  2.1923e-01,  1.2453e-01,  1.8497e-01,\n",
      "         1.9306e-01,  1.7169e-01,  1.4926e-01,  1.4328e-01,  1.7837e-01,\n",
      "         2.2950e-01,  1.7026e-01,  2.4611e+00,  2.5711e-01,  1.4745e-01,\n",
      "         1.6339e-01,  2.5688e-01,  1.6264e-01,  1.5801e-01,  1.6309e-01,\n",
      "         1.4103e-01,  1.6114e-01,  1.4158e-01,  1.7461e-01,  1.7149e-01,\n",
      "         2.7059e-01,  1.7242e-01,  1.2586e-01,  1.8509e-01,  1.6231e-01,\n",
      "         1.5975e-01,  1.6281e-01,  1.7397e-01,  1.7470e-01,  1.4098e-01,\n",
      "         2.2063e-01,  1.8709e-01,  1.8378e-01,  4.0520e-01,  1.6544e-01,\n",
      "         1.6063e-01,  1.6041e-01,  1.4579e-01,  1.6455e-01,  1.5979e-01,\n",
      "         7.1167e-01,  1.7078e-01,  1.5860e-01,  1.7149e-01,  1.6616e-01,\n",
      "         1.9971e-01,  1.5718e-01,  1.7235e-01,  1.8438e-01,  1.5239e-01,\n",
      "         1.7162e-01,  1.3605e-01,  1.5651e-01,  1.4684e-01,  1.7189e-01,\n",
      "         1.4549e-01,  1.7558e-01,  1.3983e-01,  1.9443e-01,  1.5784e-01,\n",
      "         1.5582e-01,  1.9934e-01,  1.9525e-01,  1.4703e-01,  2.4204e-01,\n",
      "         1.8359e-01,  1.5044e-01,  2.0058e-01,  1.6991e-01,  2.8148e-01,\n",
      "         1.7974e-01,  1.6118e-01,  1.7444e-01,  1.8981e-01,  1.8370e-01,\n",
      "         2.2226e-01,  1.5726e-01,  1.4730e-01,  1.6256e-01,  1.8383e-01,\n",
      "         2.4254e-01,  2.5519e-01,  1.7940e-01,  1.6245e-01,  1.4989e-01,\n",
      "         1.5379e-01,  1.9139e-01,  1.9344e-01,  1.3398e-01,  1.8337e-01,\n",
      "         1.5040e-01,  1.7911e-01,  1.6171e-01,  1.3523e-01,  5.4605e+00,\n",
      "         1.5881e-01,  1.4423e-01,  1.5600e-01,  1.4916e-01,  1.7011e-01,\n",
      "         1.9777e-01,  2.0215e-01,  1.2799e-01,  1.5923e-01,  2.1676e-01,\n",
      "         1.3977e-01,  1.7835e-01,  2.4116e-01,  1.7344e-01,  2.1488e-01,\n",
      "         1.5579e-01,  1.6865e-01,  1.7583e-01,  1.4176e-01,  1.5000e-01,\n",
      "         1.7954e-01,  1.7976e-01,  2.8477e-01,  1.5168e-01,  2.2093e-01,\n",
      "         2.0215e-01,  1.3400e-01,  1.4388e-01,  1.7456e-01,  1.5423e-01,\n",
      "         1.5707e-01,  1.5100e-01,  1.4948e-01,  1.5853e-01,  1.6074e-01,\n",
      "         1.7330e-01,  1.5760e-01,  1.4226e-01,  1.7912e-01,  1.8282e-01,\n",
      "         1.2702e-01,  1.8516e-01,  1.9899e-01,  1.6360e-01,  1.6236e-01,\n",
      "         1.5465e-01,  1.5235e-01,  1.7416e-01,  1.7015e-01,  1.5917e-01,\n",
      "         1.4212e-01,  1.6291e-01,  2.5345e-01,  1.6701e-01,  1.4763e-01,\n",
      "         1.6975e-01,  2.0256e-01,  1.6515e-01,  1.9852e-01,  1.2991e+00,\n",
      "         1.9594e-01,  2.6771e-01,  1.9862e-01,  1.5207e-01,  1.6803e-01,\n",
      "         1.5831e-01,  4.0148e+00,  1.6928e-01,  1.3382e-01,  1.7820e-01,\n",
      "         2.0096e-01,  2.1817e-01,  1.1961e-01,  1.8248e-01,  1.6298e-01,\n",
      "         1.7030e-01,  1.4106e-01,  1.6028e-01,  1.3935e-01,  1.5241e-01,\n",
      "         1.7868e-01,  2.1180e-01,  1.8998e-01,  2.3722e-01,  1.4837e-01,\n",
      "         1.7459e-01,  1.8545e-01,  2.3156e-01,  1.7222e-01,  1.5377e-01,\n",
      "         2.5178e-01,  2.8675e-01,  2.0507e-01,  2.5137e-01,  1.8816e-01,\n",
      "         1.5631e-01,  1.6928e-01,  1.6876e-01,  8.0431e+00,  2.0267e-01,\n",
      "         2.2498e-01,  1.4882e-01,  1.8083e-01,  1.8124e-01,  2.2278e-01,\n",
      "         1.7481e-01,  2.3087e-01,  1.8128e-01,  2.1446e-01,  1.4030e-01,\n",
      "         2.1781e-01,  1.4043e-01,  2.1164e-01,  2.1963e-01,  1.5551e-01,\n",
      "         1.7168e-01,  2.0861e-01,  1.8555e-01,  1.4485e-01,  1.8797e-01,\n",
      "         3.8759e-01,  2.1366e-01,  1.6399e-01,  1.6895e-01,  1.5024e-01,\n",
      "         1.6269e-01,  1.9745e-02,  1.9682e-01,  2.7993e-01,  1.9728e-01,\n",
      "         2.1243e-01,  1.5368e-01,  1.8379e-01,  1.8075e-01,  1.4863e-01,\n",
      "         2.5591e-01,  1.1998e+00,  1.5990e-01,  1.6892e-01,  1.4405e-01,\n",
      "         1.6796e-01,  1.7778e-01,  1.5223e-01,  2.2864e+00,  2.5031e-01,\n",
      "         1.6878e-01,  1.5282e-01,  1.7926e-01,  1.5576e-01,  1.9016e-01,\n",
      "         1.4083e-01,  2.2560e-01,  2.2790e-01,  1.9366e-01,  2.2626e-01,\n",
      "         1.6086e-01,  1.7701e-01,  1.4979e-01,  1.4872e-01,  1.4451e-01,\n",
      "         1.4825e-01,  1.7975e-01,  1.8818e-01,  1.7596e-01,  1.9973e-01,\n",
      "         1.6966e-01,  1.5584e-01,  4.9063e-01,  1.9658e-01,  1.8339e-01,\n",
      "         1.8400e-01,  2.9085e-01,  2.0234e-01,  1.8496e-01,  2.1258e-01,\n",
      "         1.5033e-01,  1.6261e-01,  1.8824e-01,  1.7955e-01,  1.6545e-01,\n",
      "         1.9544e-01,  2.7973e-02], requires_grad=True)\n",
      "After final layer weights update:\n",
      "  Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Before final layer weights update:\\n ', model.decoder.final_layer_norm.weight)\n",
    "\n",
    "model.decoder.final_layer_norm.weight.data.fill_(0.0)\n",
    "\n",
    "# Verify that the values have been set to zeros\n",
    "print('After final layer weights update:\\n ', model.decoder.final_layer_norm.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0079bb5-e29c-4184-bd49-bd8381ba2b7e",
   "metadata": {},
   "source": [
    "### Q6: Verify if the Q&A task works after resetting the weights of the above layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "82d97cdf-d060-4177-8e83-50b0831482d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split='validation[0:1000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "98f38db4-2de1-44f5-9e8a-b16c9328d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \n",
    "    x['prompt'] = ['Given a question and context, Answer the question using context. Question: ' + xi[0] + ' Context: ' + xi[1]  \n",
    "                 for xi in list(zip(x['question'],  x['context']))]\n",
    "    return x\n",
    "\n",
    "squad = squad.map(lambda x: preprocess(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bb056d77-7212-432e-8a56-521b934d497e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7dfd4152df4d0aad0253b4cb3cade5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = squad.map(lambda x: tokenizer(x['prompt'], padding=True, truncation=True, \n",
    "                                      return_tensors=\"pt\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a1078ceb-42d9-46c7-9f34-8af109b1ee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322973497e0046308c8e6409245c5863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(x):\n",
    "    \n",
    "    x['output'] = [model.generate(torch.tensor(x_ids).reshape(1, -1), \n",
    "                                  max_new_tokens=100, \n",
    "                                  do_sample=False)[0] for x_ids in x['input_ids']]\n",
    "    return x\n",
    "    \n",
    "squad = squad.map(lambda x: generate(x), batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f68ff0b4-dc9e-4cb4-9df2-37a4286d2698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d416701f0be4e78b1c1d8a632e6bf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = squad.map(lambda x: decode(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "753e1619-c163-4b89-ae50-004ee26b9897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 0.0}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "predictions = [{'id': t[0], 'prediction_text':t[1]} for t in list(zip(squad['id'], squad['predicted_answers']))]\n",
    "references = [{'id': t[0], 'answers':t[1]} for t in list(zip(squad['id'], squad['answers']))]\n",
    "\n",
    "squad_metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779a918-ca73-4023-b7d2-ceb4036ca96d",
   "metadata": {},
   "source": [
    "Resetting the final decoder layer weights to zero has severely degradation in the model's ability to understand and generate accurate responses to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74bb7b4-a835-4306-97f0-ba2620b4fc3a",
   "metadata": {},
   "source": [
    "### Q9: Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "21387dc7-1f15-480f-85c2-f7eba6605c2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Stack(\n",
       "  (embed_tokens): Embedding(32128, 512)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 6)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1-7): 7 x T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d221862d-9851-43f3-aa01-5f7ff08a6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dimension = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "26fa1075-364c-4e0c-8925-0c3889127a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.final_layer_norm.weight.data = model.decoder.final_layer_norm.weight.data[:new_dimension]\n",
    "for i, block in enumerate(model.decoder.block):\n",
    "    if i == 0:\n",
    "        block.layer[0].SelfAttention.o.weight.data = block.layer[0].SelfAttention.o.weight.data[:new_dimension, :]\n",
    "        continue\n",
    "    \n",
    "    # Self-Attention Layer\n",
    "    block.layer[0].SelfAttention.q.weight.data = block.layer[0].SelfAttention.q.weight.data[:, :new_dimension]\n",
    "    block.layer[0].SelfAttention.k.weight.data = block.layer[0].SelfAttention.k.weight.data[:, :new_dimension]\n",
    "    block.layer[0].SelfAttention.v.weight.data = block.layer[0].SelfAttention.v.weight.data[:, :new_dimension]\n",
    "    block.layer[0].SelfAttention.o.weight.data = block.layer[0].SelfAttention.o.weight.data[:new_dimension, :]\n",
    "\n",
    "    # Cross-Attention Layer\n",
    "    block.layer[1].EncDecAttention.q.weight.data = block.layer[1].EncDecAttention.q.weight.data[:, :new_dimension]\n",
    "    block.layer[1].EncDecAttention.k.weight.data = block.layer[1].EncDecAttention.k.weight.data[:, :new_dimension]\n",
    "    block.layer[1].EncDecAttention.v.weight.data = block.layer[1].EncDecAttention.v.weight.data[:, :new_dimension]\n",
    "    block.layer[1].EncDecAttention.o.weight.data = block.layer[1].EncDecAttention.o.weight.data[:new_dimension, :]\n",
    "\n",
    "    # Dense Relu Layer\n",
    "    block.layer[2].DenseReluDense.wi_0.weight.data = block.layer[2].DenseReluDense.wi_0.weight.data[:, :new_dimension]\n",
    "    block.layer[2].DenseReluDense.wi_1.weight.data = block.layer[2].DenseReluDense.wi_1.weight.data[:, :new_dimension]\n",
    "    block.layer[2].DenseReluDense.wo.weight.data = block.layer[2].DenseReluDense.wo.weight.data[:new_dimension, :]\n",
    "    \n",
    "    # Layer Norm\n",
    "    block.layer[2].layer_norm.weight.data = block.layer[2].layer_norm.weight.data[:new_dimension]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576029f8-9b08-4931-9736-29383bdd2fc1",
   "metadata": {},
   "source": [
    "### Reload the original google/flan-t5-small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a877e98e-d52c-4a78-b083-0fd7c1c4d62b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: filelock in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.24.1\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe26ad8-df01-45d3-b1a6-f1ea4f2ecb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2b2e6a9-8663-400d-a0e9-d3bc42429062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963070f-de1f-48ca-86e1-9944ea446b16",
   "metadata": {},
   "source": [
    "### Q11: Train the model for a Q&A task that takes a context as addi'onal input along with the ques'on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e5c2646-799f-46a5-9b91-645e218c14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    questions =[q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "    answers = [\" or \".join(a['text']) for a in examples[\"answers\"]]\n",
    "    answers = [a.strip() for a in answers]\n",
    "    \n",
    "    l = list(zip(questions, contexts, answers))\n",
    "    instr = \"This is QA task where given a question, you need to answer it strictly using the context. \"\n",
    "    examples['prompt'] = [instr + \" \\n Question is: \\n \" + i[0] + \" \\n Context is: \\n \" + i[1] + \" \\n Answer is : \\n \" for i in l]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab26738a-95b0-462d-a23c-06eb4fd30c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/drpawar/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Sat Nov 25 11:52:51 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /Users/drpawar/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Sat Nov 25 11:52:51 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5277c2d6f1c447e9d937333673b84e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split='train[0:5000]')\n",
    "\n",
    "p_squad = squad.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f1f6649-7b80-449e-bcb6-05f9b0a93bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce1a955f7bc4352a56945c39ad9c480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split='train[5001:8000]')\n",
    "\n",
    "p_squad_val = squad.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e20355e-a7f2-4d75-b19f-fb87db087a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e6c224ca8947c89c1c3b32eff976c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54056000623b408fa1f240fefe7e092a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_squad = p_squad.map(lambda x: tokenizer(x['prompt'], padding=True, truncation=True,\n",
    "                                          return_tensors=\"pt\"), batched=True)\n",
    "p_squad_val = p_squad_val.map(lambda x: tokenizer(x['prompt'], padding=True, truncation=True, \n",
    "                                                  return_tensors=\"pt\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de94ba8e-5d94-4d56-86d5-3ed1796f9553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7e8114c2034538939af9e726ffd6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98f83b7b5a7434cbd55574cf810312d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_labels(x):\n",
    "    answers = [\" or \".join(a['text']) for a in x[\"answers\"]]\n",
    "    answers = [a.strip() for a in answers]\n",
    "    labels = tokenizer(text_target=answers, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    x[\"labels\"] = labels[\"input_ids\"]\n",
    "    return x\n",
    "    \n",
    "p_squad = p_squad.map(lambda x: get_labels(x), batched=True)\n",
    "p_squad_val = p_squad_val.map(lambda x: get_labels(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8438e3ce-4696-44de-983c-24e676dc947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages/transformers/training_args.py:1843: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./finetuning_output',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    # logging & evaluation strategies\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=False,\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f169f95-c8d6-4cbb-b468-d6636164c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cdfb90f-ceee-4bb8-97f7-632e7c562765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8e6f36e-8283-4a68-bd77-e250fe3f5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=p_squad,\n",
    "    eval_dataset=p_squad_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "beb66569-a792-4435-8aac-bef6a2e890ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 28:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.411700</td>\n",
       "      <td>0.079908</td>\n",
       "      <td>0.766433</td>\n",
       "      <td>0.472363</td>\n",
       "      <td>0.765357</td>\n",
       "      <td>0.765771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.039436</td>\n",
       "      <td>0.803508</td>\n",
       "      <td>0.502198</td>\n",
       "      <td>0.802617</td>\n",
       "      <td>0.803051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.038852</td>\n",
       "      <td>0.805777</td>\n",
       "      <td>0.501293</td>\n",
       "      <td>0.804930</td>\n",
       "      <td>0.805163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/drpawar/work/envs/py3.8_accruals_v2/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=1.2329924387613933, metrics={'train_runtime': 1710.9676, 'train_samples_per_second': 8.767, 'train_steps_per_second': 1.096, 'total_flos': 2788357570560000.0, 'train_loss': 1.2329924387613933, 'epoch': 3.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc3b2253-d57e-4800-9bc1-91732e7a833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d687acfd-abac-4462-a24a-3521717495dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "finetuned_model_mps = finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93e4db95-f164-4135-91ac-84ad77f6b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split='validation[0:1000]')\n",
    "\n",
    "squad = squad.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cea17310-9894-41e0-b0e9-5e328af159ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = squad.map(lambda x: tokenizer(x['prompt'], padding=True, truncation=True, \n",
    "                                      return_tensors=\"pt\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89997910-8994-43ea-82a6-62737e6e5212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c04e874c9ad48e6840c40862ef1a7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "def generate(x):\n",
    "    \n",
    "    x['output'] = [finetuned_model_mps.generate(torch.tensor(x_ids, device='mps').reshape(1, -1), \n",
    "                                  max_new_tokens=100, \n",
    "                                  do_sample=False)[0] for x_ids in x['input_ids']]\n",
    "    return x\n",
    "    \n",
    "squad = squad.map(lambda x: generate(x), batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e063aef-b34c-44fd-9617-31b05847d1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a74ce67c724febbeeace21102ad87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode(x):\n",
    "    x['predicted_answers'] = [tokenizer.decode(torch.tensor(xo), skip_special_tokens=True) for xo in x['output']]\n",
    "    return x\n",
    "\n",
    "squad = squad.map(lambda x: decode(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2bf1a0cb-8cf3-4409-b896-66bec88c24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "squad_metric = evaluate.load('squad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a011cb1e-7623-4c11-add3-0a3424ae7568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 76.7, 'f1': 82.45654752265274}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [{'id': t[0], 'prediction_text':t[1]} for t in list(zip(squad['id'], squad['predicted_answers']))]\n",
    "references = [{'id': t[0], 'answers':t[1]} for t in list(zip(squad['id'], squad['answers']))]\n",
    "\n",
    "squad_metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dddab1-ef49-41cd-ac43-11db94177e41",
   "metadata": {},
   "source": [
    "The fine-tuned model has shown a notable enhancement in both the Exact Match and F1 Score compared to the pretrained model, indicating that the fine-tuning process has effectively improved the model's performance on the SQuAD dataset.\n",
    "\n",
    "Achieving an EM score of 76.7% is considered quite good in the context of SQuAD, where providing the exact correct answer can be challenging due to the diversity and complexity of questions.\n",
    "\n",
    "The F1 score of 82.46% demonstrates that the fine-tuned model is not only accurate in terms of exact matches but also excels in providing answers that closely align with the ground truth answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d96c3-3d6d-4f63-b08f-265cd03a6571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
